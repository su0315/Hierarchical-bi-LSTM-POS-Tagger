{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1prcdoUugyV6JV_sTa1Wi9iipQRqU8BU1",
      "authorship_tag": "ABX9TyNwLKOy3phnFypV1pzx5PHw"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xNst021NxUUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1cf806-c48d-4393-ec23-8a60d12ef824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 112 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 126 kB 3.9 MB/s \n",
            "\u001b[?25h  Building wheel for polyglot (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q polyglot\n",
        "!pip install -q conllu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "from polyglot.mapping import Embedding\n",
        "from conllu import parse\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import random"
      ],
      "metadata": {
        "id": "ZoX47ks-E1WF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/DL for NLP project/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLNhu2-IHNgx",
        "outputId": "178ce054-1af7-49cb-b88b-bfc9cac2dc94"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/DL for NLP project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "BPWbuouYFggs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=100)\n",
        "        self.bilstm = nn.LSTM(input_size=100, hidden_size=100, num_layers=1, bidirectional=True)\n",
        "\n",
        "    def forward(self, chars: torch.tensor):  # chars = (N_CHARS,)\n",
        "        embedded = self.embedding(chars)\n",
        "        _, (final_hidden, _) = self.bilstm(embedded.view(len(chars), 1, 100))\n",
        "        return final_hidden.view(-1)  # (2*HIDDEN,)"
      ],
      "metadata": {
        "id": "dzXtjgg2IHgN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class POS_Tagger(nn.Module):\n",
        "\n",
        "    def __init__(self, model_type, use_polyglot, use_freqbin, embedding_matrix, c_vocab_size, b_vocab_size, freq_max, noise):\n",
        "        super(POS_Tagger, self).__init__()\n",
        "        self.model_type = model_type\n",
        "        self.use_polyglot = use_polyglot\n",
        "        self.use_freqbin = use_freqbin\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix).requires_grad_(True)\n",
        "        self.characterbased = Encoder(c_vocab_size).to(device)\n",
        "        self.bytebased = Encoder(b_vocab_size).to(device)\n",
        "        self.input_size = 0\n",
        "        if 'w' in model_type:\n",
        "            self.input_size += 64 if use_polyglot else 128\n",
        "        if 'c' in model_type:\n",
        "            self.input_size += 200\n",
        "        if 'b' in model_type:\n",
        "            self.input_size += 200\n",
        "        self.bilstm = nn.LSTM(input_size=self.input_size, hidden_size=100, num_layers=1, bidirectional=True)\n",
        "        self.pos_tagger = nn.Linear(in_features=200, out_features=17)\n",
        "        self.freqbin = nn.Linear(in_features=200, out_features=freq_max)\n",
        "        self.noise = noise\n",
        "\n",
        "    def forward(self, tokens: torch.tensor, char_lists: list,\n",
        "                byte_lists: list):  # tokens = (N_TOKENS, 64), char_lists = List[List[int]]\n",
        "        concatted = torch.zeros((len(tokens), self.input_size), device=device)  # concatted = (N_TOKENS, 264)\n",
        "        if 'w' in self.model_type:\n",
        "            embedded_words = self.embedding(tokens)\n",
        "        for i, (char_list, byte_list) in enumerate(zip(char_lists, byte_lists)):\n",
        "            embedded = torch.zeros((0,), device=device)\n",
        "            if 'w' in self.model_type:\n",
        "                embedded = torch.concat((embedded, embedded_words[i]))\n",
        "            if 'c' in self.model_type:\n",
        "                embedded_characters = self.characterbased(torch.tensor(char_list, device=device))\n",
        "                embedded = torch.concat((embedded, embedded_characters))\n",
        "            if 'b' in self.model_type:\n",
        "                embedded_bytes = self.bytebased(torch.tensor(byte_list, device=device))\n",
        "                embedded = torch.concat((embedded, embedded_bytes))\n",
        "            concatted[i] = embedded\n",
        "        if self.training:\n",
        "            noise = torch.autograd.Variable(concatted.data.new(concatted.size()).normal_(0, self.noise))\n",
        "            concatted = concatted + noise\n",
        "        bilstm_out, _ = self.bilstm(concatted.view(len(tokens), 1, self.input_size))\n",
        "        pos_tags = self.pos_tagger(bilstm_out.view(len(tokens), 200))\n",
        "        if self.use_freqbin:\n",
        "            freq = self.freqbin(bilstm_out.view(len(tokens), 200))\n",
        "            return pos_tags, freq\n",
        "        else:\n",
        "            return pos_tags, None"
      ],
      "metadata": {
        "id": "qdAh5E-UT69_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Main:\n",
        "\n",
        "    def __init__(self, language, model_type=('w', 'c'), polyglot=False, freqbin=False):\n",
        "        with open(f\"languages/{language}/{language}-ud-train.conllu\") as file:\n",
        "            self.train_data = parse(file.read())\n",
        "            random.seed(0)\n",
        "            self.train_data = random.sample(self.train_data, len(self.train_data))[:5000]\n",
        "        # with open(f\"languages/{language}/{language}-ud-dev.conllu\") as file:\n",
        "        #     self.dev_data = parse(file.read())\n",
        "        with open(f\"languages/{language}/{language}-ud-test.conllu\") as file:\n",
        "            self.test_data = parse(file.read())\n",
        "        self.embeds = Embedding.from_glove(f\"polyglot/{language}.polyglot.txt\")\n",
        "\n",
        "        self.n_epochs = 20\n",
        "        self.report_every = 21\n",
        "        self.learning_rate = 0.1\n",
        "        self.noise = 0.2\n",
        "\n",
        "        self.model_type = model_type\n",
        "        self.polyglot = polyglot\n",
        "        self.freqbin = freqbin\n",
        "\n",
        "    def build_indexes(self):\n",
        "        self.w2i = {}\n",
        "        self.c2i = {}\n",
        "        self.b2i = {}\n",
        "        self.l2i = {}\n",
        "        self.w2i[\"_UNK\"] = 0\n",
        "        self.c2i[\"_UNK\"] = 0\n",
        "        self.c2i[\"<w>\"] = 1\n",
        "        self.c2i[\"</w>\"] = 2\n",
        "        self.b2i[\"_UNK\"] = 0\n",
        "        self.b2i[\"<w>\"] = 1\n",
        "        self.b2i[\"</w>\"] = 2\n",
        "        self.l2i = {\"ADJ\": 0, \"ADP\": 1, \"ADV\": 2, \"AUX\": 3, \"CONJ\": 4, \"DET\": 5,\n",
        "                    \"INTJ\": 6, \"NOUN\": 7, \"NUM\": 8, \"PART\": 9, \"PRON\": 10,\n",
        "                    \"PROPN\": 11, \"PUNCT\": 12, \"SCONJ\": 13, \"SYM\": 14, \"VERB\": 15,\n",
        "                    \"X\": 16}\n",
        "        self.freqbin_dict = {}\n",
        "        tokens = []\n",
        "        for sentence in self.train_data:\n",
        "            for token in sentence:\n",
        "                if type(token['id']) != int:\n",
        "                    continue\n",
        "                word = token['form'].lower()\n",
        "                if word not in self.w2i:\n",
        "                    self.w2i[word] = len(self.w2i)\n",
        "                for character in word:\n",
        "                    if character not in self.c2i:\n",
        "                        self.c2i[character] = len(self.c2i)\n",
        "                    for byte in character.encode(\"utf-8\"):\n",
        "                        if byte not in self.b2i:\n",
        "                            self.b2i[byte] = len(self.b2i)\n",
        "                tokens.append(token['form'].lower())\n",
        "        for word, frequency in Counter(tokens).items():\n",
        "            self.freqbin_dict[word] = int(np.log(frequency))\n",
        "\n",
        "        if self.polyglot:\n",
        "            self.embedding_matrix = torch.FloatTensor(size=(len(self.w2i), 64))\n",
        "            for word, index in self.w2i.items():\n",
        "                embedding = self.embeds.get(word.lower())\n",
        "                if self.polyglot and embedding is not None:\n",
        "                    self.embedding_matrix[index] = torch.FloatTensor(embedding)\n",
        "                else:\n",
        "                    self.embedding_matrix[index] = torch.rand((1, 64))\n",
        "        else:\n",
        "            self.embedding_matrix = torch.rand((len(self.w2i), 128))\n",
        "\n",
        "    def tensorize_data(self, sentence):\n",
        "        tokens_list = []\n",
        "        char_lists = []\n",
        "        byte_lists = []\n",
        "        pos_tag_list = []\n",
        "        freq_list = []\n",
        "        for token in sentence:\n",
        "            if type(token['id']) != int:\n",
        "                continue\n",
        "            word = token['form'].lower()\n",
        "            tokens_list.append(self.w2i[word] if word in self.w2i else 0)\n",
        "            char_list = [self.c2i['<w>']]\n",
        "            byte_list = [self.b2i['<w>']]\n",
        "            for char in word:\n",
        "                char_list.append(self.c2i[char] if char in self.c2i else 0)\n",
        "                for byte in char.encode('utf-8'):\n",
        "                    byte_list.append(self.b2i[byte] if byte in self.b2i else 0)\n",
        "            char_list.append(self.c2i['</w>'])\n",
        "            byte_list.append(self.b2i['</w>'])\n",
        "            char_lists.append(char_list)\n",
        "            byte_lists.append(byte_list)\n",
        "            pos_tag_list.append(self.l2i[token['upos']])\n",
        "            freq_list.append(self.freqbin_dict[word] if word in self.freqbin_dict else 0)\n",
        "        tokens = torch.LongTensor(tokens_list).to(device)\n",
        "        pos_gold = torch.LongTensor(pos_tag_list).to(device)\n",
        "        freq_gold = torch.LongTensor(freq_list).to(device)\n",
        "        return tokens, char_lists, byte_lists, pos_gold, freq_gold\n",
        "\n",
        "    def eval(self, data):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            accuracy = 0\n",
        "            n_tokens = 0\n",
        "            for sentence in tqdm(data, desc=\"Evaluation\"):\n",
        "                tokens, char_lists, byte_lists, golden, _ = self.tensorize_data(sentence)\n",
        "                pred, _ = self.model(tokens, char_lists, byte_lists)\n",
        "                pred_label = torch.argmax(pred, dim=1)\n",
        "                accuracy += torch.sum(pred_label == golden)\n",
        "                n_tokens += len(tokens)\n",
        "        return accuracy / n_tokens\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.model = POS_Tagger(self.model_type, self.polyglot, self.freqbin, self.embedding_matrix, \n",
        "                                len(self.c2i), len(self.b2i), max(self.freqbin_dict.values()) + 1, self.noise\n",
        "                                ).to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(self.model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            total_loss = 0\n",
        "            self.model.train()\n",
        "            for i, sentence in enumerate(tqdm(self.train_data, desc=\"Training  \")):\n",
        "                optimizer.zero_grad()\n",
        "                tokens, char_lists, byte_lists, pos_gold, freq_gold = self.tensorize_data(sentence)\n",
        "                pos_pred, freq_pred = self.model(tokens, char_lists, byte_lists)\n",
        "                loss = criterion(pos_pred, pos_gold)\n",
        "                if self.freqbin:\n",
        "                    loss += criterion(freq_pred[:-1], freq_gold[1:])\n",
        "                total_loss += loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Testing\n",
        "            if ((epoch + 1) % self.report_every) == 0:\n",
        "                train_accuracy = self.eval(self.train_data)\n",
        "                dev_accuracy = self.eval(self.dev_data)\n",
        "                loss = total_loss / len(self.train_data)\n",
        "                print(f\"epoch: {epoch}, loss: {loss:.4f}, train acc: {train_accuracy:.4f}, dev acc: {dev_accuracy:.4f}\")\n",
        "\n",
        "    def test(self):\n",
        "        test_accuracy = self.eval(self.test_data)\n",
        "        print(f\"\\nTest accuracy: {test_accuracy}\")\n",
        "        return test_accuracy"
      ],
      "metadata": {
        "id": "p9Q6SbOxXSx9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_name(model):\n",
        "    out = '+'.join(model['model_type'])\n",
        "    if model['polyglot']:\n",
        "        out += '_p'\n",
        "    if model['freqbin']:\n",
        "        out += '_f'\n",
        "    return out"
      ],
      "metadata": {
        "id": "GG2_Q3iS_bKX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "        # {'model_type': ('w',), 'polyglot': False, 'freqbin': False},\n",
        "        # {'model_type': ('c',), 'polyglot': False, 'freqbin': False},\n",
        "        # {'model_type': ('c', 'b'), 'polyglot': False, 'freqbin': False},\n",
        "        # {'model_type': ('w', 'c'), 'polyglot': False, 'freqbin': False},\n",
        "        # {'model_type': ('w', 'c'), 'polyglot': True, 'freqbin': False},\n",
        "        {'model_type': ('w', 'c'), 'polyglot': True, 'freqbin': True},\n",
        "        ]\n",
        "languages = ['ar', 'bg', 'cs', 'da', 'de', 'en', 'es', 'eu', 'fa', 'fi', 'fr',\n",
        "             'he', 'hi', 'hr', 'id', 'it', 'nl', 'no', 'pl', 'pt', 'sl', 'sv']\n",
        "with open(\"results.csv\", 'w') as file:\n",
        "    file.write(f\"Language, Model, Accuracy, Time\\n\")\n",
        "with open(\"results.csv\", 'a', 1) as file:\n",
        "    for language in reversed(languages):\n",
        "        print(f\"Working with language {language}\")\n",
        "        for model in models:\n",
        "            print(f\"\\tTraining model {model_name(model)}\")\n",
        "            pos_tagger = Main(language, **model)\n",
        "            pos_tagger.build_indexes()\n",
        "            start = time.time()\n",
        "            pos_tagger.train()\n",
        "            end = time.time()\n",
        "            file.write(f\"{language}, {model_name(model)}, {pos_tagger.test()}, {end - start}\\n\")\n",
        "            torch.save(pos_tagger.model.state_dict(), f\"models/{model_name(model)}_{language}.pt\")"
      ],
      "metadata": {
        "id": "uxW2uRQjPz8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}